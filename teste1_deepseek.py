import requests

# Defina a URL do servidor vLLM rodando localmente
API_URL = "http://localhost:8123/v1/chat/completions"

# Corpo da requisiÃ§Ã£o no formato OpenAI
payload = {
    "model": "TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ",
    "messages": [
        {"role": "system", "content": "VocÃª Ã© um assistÃªnte Ãºtil e educado."},
        {"role": "user", "content": "O que sÃ£o LLMs?"}
    ],
    "max_tokens": 400,
    "temperature": 0.7
}

# CabeÃ§alhos HTTP
headers = {"Content-Type": "application/json"}

# Faz a requisiÃ§Ã£o para a API
response = requests.post(API_URL, json=payload, headers=headers)

# Exibe a resposta formatada
if response.status_code == 200:
    resposta = response.json()
    print("\nðŸ¤– Resposta do LLM:\n\n")
    print(resposta["choices"][0]["message"]["content"])
    print("\n\nðŸ¤– Fim da respostas do LLM.\n")
else:
    print("Erro:", response.status_code, response.text)

